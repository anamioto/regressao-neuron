{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regressao_na_Pratica_Bootcamp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTHRVYthVbS5aYC4T42rme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicholasrichers/regressao-neuron/blob/master/Regressao_na_Pratica_Bootcamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyI_qSqnecfB",
        "colab_type": "text"
      },
      "source": [
        "# Regressão na prática"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pt1NIctehSj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*(introdução básica sobre como será o curso... as anotações nessa área servirão para nos ajudar a guiar a aula e nao esquecer nenhum detalhe importante, e deixar como registro posterior para o aluno)*\n",
        "\n",
        "\n",
        "- Quando aplicar? (pode usar o daset abaixo para explicar)\n",
        "- Diferenciação de técnicas de regressão em relação a de classificação\n",
        "- Introdução a técnicas de regressão (seção seguinte)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i1V2_4RY8xN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuN1rNPQI6mP",
        "colab_type": "text"
      },
      "source": [
        "## Conjunto de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akCG5gfyI-Pt",
        "colab_type": "text"
      },
      "source": [
        "Nesse treinamento vamos usar um dataset muito conhecido na literatura, chamado [boston housing](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), que usa dados demograficos/socioeconômicos da região para prever a mediana dos preços das casas em casa região da cidade (revisar)\n",
        "\n",
        "\n",
        "\n",
        "-> traduzir a descrição das colunas\n",
        "\n",
        "- CRIM - per capita crime rate by town\n",
        "- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "- INDUS - proportion of non-retail business acres per town.\n",
        "- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
        "- NOX - nitric oxides concentration (parts per 10 million)\n",
        "- RM - average number of rooms per dwelling\n",
        "- AGE - proportion of owner-occupied units built prior to 1940\n",
        "- DIS - weighted distances to five Boston employment centres\n",
        "- RAD - index of accessibility to radial highways\n",
        "- TAX - full-value property-tax rate per USD10,000\n",
        "- PTRATIO - pupil-teacher ratio by town\n",
        "- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "- LSTAT - % lower status of the population\n",
        "- MEDV (target) - Median value of owner-occupied homes in USD1000's (em milhares)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuvRv-1x3uIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#impotando a biblioteca pandas\n",
        "import pandas as pd"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBO_OlXrj7jd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bd1fced-9ea8-46d5-bc4d-7ae125291812"
      },
      "source": [
        "#url onde o dataset está armazenado\n",
        "url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
        "\n",
        "#importando o dataset para o pandas\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "#dimensão do dataset\n",
        "print(\"O dataset possui {} linhas (registros) e {} colunas (features)\".format(df.shape[0], df.shape[1]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O dataset possui 506 linhas (registros) e 14 colunas (features)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9DPXmfhZzwz",
        "colab_type": "text"
      },
      "source": [
        "Uma amostra do dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnDSHjyxZDDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9582cd35-5b1e-4c4b-dcb2-357c70529ac4"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crim</th>\n",
              "      <th>zn</th>\n",
              "      <th>indus</th>\n",
              "      <th>chas</th>\n",
              "      <th>nox</th>\n",
              "      <th>rm</th>\n",
              "      <th>age</th>\n",
              "      <th>dis</th>\n",
              "      <th>rad</th>\n",
              "      <th>tax</th>\n",
              "      <th>ptratio</th>\n",
              "      <th>b</th>\n",
              "      <th>lstat</th>\n",
              "      <th>medv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      crim    zn  indus  chas    nox  ...  tax  ptratio       b  lstat  medv\n",
              "0  0.00632  18.0   2.31     0  0.538  ...  296     15.3  396.90   4.98  24.0\n",
              "1  0.02731   0.0   7.07     0  0.469  ...  242     17.8  396.90   9.14  21.6\n",
              "2  0.02729   0.0   7.07     0  0.469  ...  242     17.8  392.83   4.03  34.7\n",
              "3  0.03237   0.0   2.18     0  0.458  ...  222     18.7  394.63   2.94  33.4\n",
              "4  0.06905   0.0   2.18     0  0.458  ...  222     18.7  396.90   5.33  36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Detv1bMhF-q3",
        "colab_type": "text"
      },
      "source": [
        "Descrição das variáveis **numéricas**. *(se nao for relevante pode apagar)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfL8m6OuF-HQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "b3c695ea-2b83-4409-c41c-6e2d35aaf21d"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crim</th>\n",
              "      <th>zn</th>\n",
              "      <th>indus</th>\n",
              "      <th>chas</th>\n",
              "      <th>nox</th>\n",
              "      <th>rm</th>\n",
              "      <th>age</th>\n",
              "      <th>dis</th>\n",
              "      <th>rad</th>\n",
              "      <th>tax</th>\n",
              "      <th>ptratio</th>\n",
              "      <th>b</th>\n",
              "      <th>lstat</th>\n",
              "      <th>medv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.613524</td>\n",
              "      <td>11.363636</td>\n",
              "      <td>11.136779</td>\n",
              "      <td>0.069170</td>\n",
              "      <td>0.554695</td>\n",
              "      <td>6.284634</td>\n",
              "      <td>68.574901</td>\n",
              "      <td>3.795043</td>\n",
              "      <td>9.549407</td>\n",
              "      <td>408.237154</td>\n",
              "      <td>18.455534</td>\n",
              "      <td>356.674032</td>\n",
              "      <td>12.653063</td>\n",
              "      <td>22.532806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.601545</td>\n",
              "      <td>23.322453</td>\n",
              "      <td>6.860353</td>\n",
              "      <td>0.253994</td>\n",
              "      <td>0.115878</td>\n",
              "      <td>0.702617</td>\n",
              "      <td>28.148861</td>\n",
              "      <td>2.105710</td>\n",
              "      <td>8.707259</td>\n",
              "      <td>168.537116</td>\n",
              "      <td>2.164946</td>\n",
              "      <td>91.294864</td>\n",
              "      <td>7.141062</td>\n",
              "      <td>9.197104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.006320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.385000</td>\n",
              "      <td>3.561000</td>\n",
              "      <td>2.900000</td>\n",
              "      <td>1.129600</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>187.000000</td>\n",
              "      <td>12.600000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>1.730000</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.082045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.190000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.449000</td>\n",
              "      <td>5.885500</td>\n",
              "      <td>45.025000</td>\n",
              "      <td>2.100175</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>279.000000</td>\n",
              "      <td>17.400000</td>\n",
              "      <td>375.377500</td>\n",
              "      <td>6.950000</td>\n",
              "      <td>17.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.256510</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.690000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>6.208500</td>\n",
              "      <td>77.500000</td>\n",
              "      <td>3.207450</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>330.000000</td>\n",
              "      <td>19.050000</td>\n",
              "      <td>391.440000</td>\n",
              "      <td>11.360000</td>\n",
              "      <td>21.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.677082</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>18.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.624000</td>\n",
              "      <td>6.623500</td>\n",
              "      <td>94.075000</td>\n",
              "      <td>5.188425</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>666.000000</td>\n",
              "      <td>20.200000</td>\n",
              "      <td>396.225000</td>\n",
              "      <td>16.955000</td>\n",
              "      <td>25.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>88.976200</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>27.740000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.871000</td>\n",
              "      <td>8.780000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>12.126500</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>711.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>396.900000</td>\n",
              "      <td>37.970000</td>\n",
              "      <td>50.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             crim          zn       indus  ...           b       lstat        medv\n",
              "count  506.000000  506.000000  506.000000  ...  506.000000  506.000000  506.000000\n",
              "mean     3.613524   11.363636   11.136779  ...  356.674032   12.653063   22.532806\n",
              "std      8.601545   23.322453    6.860353  ...   91.294864    7.141062    9.197104\n",
              "min      0.006320    0.000000    0.460000  ...    0.320000    1.730000    5.000000\n",
              "25%      0.082045    0.000000    5.190000  ...  375.377500    6.950000   17.025000\n",
              "50%      0.256510    0.000000    9.690000  ...  391.440000   11.360000   21.200000\n",
              "75%      3.677082   12.500000   18.100000  ...  396.225000   16.955000   25.000000\n",
              "max     88.976200  100.000000   27.740000  ...  396.900000   37.970000   50.000000\n",
              "\n",
              "[8 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKdzJGc-K8EG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaoJqiO-JeIu",
        "colab_type": "text"
      },
      "source": [
        "> **Train/Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55eF1V_0JXjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Armazenando todas as colunas do dataframe com exceção da última numa lista\n",
        "features = df.columns[:-1]\n",
        "\n",
        "#X será todas as colunas com excecão do target\n",
        "X = df[features]\n",
        "\n",
        "#y é a nossa variável target\n",
        "y = df[\"medv\"]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0WIFhCyKEn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqFdduNiZs7R",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0alt7Pbe_fi",
        "colab_type": "text"
      },
      "source": [
        "## Tipos de regressão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0fz22AIfo5C",
        "colab_type": "text"
      },
      "source": [
        "### Linear simples\n",
        "\n",
        "*(incluir aqui uma explicação teorica bem básica de regressão linear simples, tente incluir algumas formulas usando markdown, veja nesse [guia](https://colab.research.google.com/notebooks/markdown_guide.ipynb), inclua também as fontes, pra esse assunto, gosto [desse blog](https://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning/), mas use a fonte que vc preferir)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH499D8MKylh",
        "colab_type": "text"
      },
      "source": [
        "> **Prevendo o preço das casas a partir de uma variável**\n",
        "\n",
        "*(eu faria uma regressao simples, entre uma das colunas do dataset com o target, use a feature mais correlacionada ou uma que a descricao pareça fazer mais sentido)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b0-EEhePL4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4bcfe072-af0f-4882-c309-cc471d71b99f"
      },
      "source": [
        "#quando trabalhamos com apenas uma coluna, precisamos transformar em array do tipo (n_samples, 1)\n",
        "X_Rooms_train  = X_train[\"rm\"].values.reshape(-1,1)\n",
        "X_Rooms_test  = X_test[\"rm\"].values.reshape(-1,1)\n",
        "\n",
        "\n",
        "#Apenas a coluna RM\n",
        "X_Rooms_test[:5]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.416],\n",
              "       [6.758],\n",
              "       [5.983],\n",
              "       [6.065],\n",
              "       [6.297]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzuKgzneWGmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#crie um exemplo na mao que seja natural de ler usando as formulas acima\n",
        "\n",
        "#y = a*x +b "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrFKv52oafOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "92bf42cb-a98d-4984-8f8d-b85b224398b0"
      },
      "source": [
        "df.corr() ## RM x medv APAGAR DEPOIS"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crim</th>\n",
              "      <th>zn</th>\n",
              "      <th>indus</th>\n",
              "      <th>chas</th>\n",
              "      <th>nox</th>\n",
              "      <th>rm</th>\n",
              "      <th>age</th>\n",
              "      <th>dis</th>\n",
              "      <th>rad</th>\n",
              "      <th>tax</th>\n",
              "      <th>ptratio</th>\n",
              "      <th>b</th>\n",
              "      <th>lstat</th>\n",
              "      <th>medv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>crim</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.200469</td>\n",
              "      <td>0.406583</td>\n",
              "      <td>-0.055892</td>\n",
              "      <td>0.420972</td>\n",
              "      <td>-0.219247</td>\n",
              "      <td>0.352734</td>\n",
              "      <td>-0.379670</td>\n",
              "      <td>0.625505</td>\n",
              "      <td>0.582764</td>\n",
              "      <td>0.289946</td>\n",
              "      <td>-0.385064</td>\n",
              "      <td>0.455621</td>\n",
              "      <td>-0.388305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zn</th>\n",
              "      <td>-0.200469</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.533828</td>\n",
              "      <td>-0.042697</td>\n",
              "      <td>-0.516604</td>\n",
              "      <td>0.311991</td>\n",
              "      <td>-0.569537</td>\n",
              "      <td>0.664408</td>\n",
              "      <td>-0.311948</td>\n",
              "      <td>-0.314563</td>\n",
              "      <td>-0.391679</td>\n",
              "      <td>0.175520</td>\n",
              "      <td>-0.412995</td>\n",
              "      <td>0.360445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>indus</th>\n",
              "      <td>0.406583</td>\n",
              "      <td>-0.533828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.062938</td>\n",
              "      <td>0.763651</td>\n",
              "      <td>-0.391676</td>\n",
              "      <td>0.644779</td>\n",
              "      <td>-0.708027</td>\n",
              "      <td>0.595129</td>\n",
              "      <td>0.720760</td>\n",
              "      <td>0.383248</td>\n",
              "      <td>-0.356977</td>\n",
              "      <td>0.603800</td>\n",
              "      <td>-0.483725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chas</th>\n",
              "      <td>-0.055892</td>\n",
              "      <td>-0.042697</td>\n",
              "      <td>0.062938</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.091203</td>\n",
              "      <td>0.091251</td>\n",
              "      <td>0.086518</td>\n",
              "      <td>-0.099176</td>\n",
              "      <td>-0.007368</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>-0.121515</td>\n",
              "      <td>0.048788</td>\n",
              "      <td>-0.053929</td>\n",
              "      <td>0.175260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nox</th>\n",
              "      <td>0.420972</td>\n",
              "      <td>-0.516604</td>\n",
              "      <td>0.763651</td>\n",
              "      <td>0.091203</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.302188</td>\n",
              "      <td>0.731470</td>\n",
              "      <td>-0.769230</td>\n",
              "      <td>0.611441</td>\n",
              "      <td>0.668023</td>\n",
              "      <td>0.188933</td>\n",
              "      <td>-0.380051</td>\n",
              "      <td>0.590879</td>\n",
              "      <td>-0.427321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rm</th>\n",
              "      <td>-0.219247</td>\n",
              "      <td>0.311991</td>\n",
              "      <td>-0.391676</td>\n",
              "      <td>0.091251</td>\n",
              "      <td>-0.302188</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.240265</td>\n",
              "      <td>0.205246</td>\n",
              "      <td>-0.209847</td>\n",
              "      <td>-0.292048</td>\n",
              "      <td>-0.355501</td>\n",
              "      <td>0.128069</td>\n",
              "      <td>-0.613808</td>\n",
              "      <td>0.695360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0.352734</td>\n",
              "      <td>-0.569537</td>\n",
              "      <td>0.644779</td>\n",
              "      <td>0.086518</td>\n",
              "      <td>0.731470</td>\n",
              "      <td>-0.240265</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.747881</td>\n",
              "      <td>0.456022</td>\n",
              "      <td>0.506456</td>\n",
              "      <td>0.261515</td>\n",
              "      <td>-0.273534</td>\n",
              "      <td>0.602339</td>\n",
              "      <td>-0.376955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dis</th>\n",
              "      <td>-0.379670</td>\n",
              "      <td>0.664408</td>\n",
              "      <td>-0.708027</td>\n",
              "      <td>-0.099176</td>\n",
              "      <td>-0.769230</td>\n",
              "      <td>0.205246</td>\n",
              "      <td>-0.747881</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.494588</td>\n",
              "      <td>-0.534432</td>\n",
              "      <td>-0.232471</td>\n",
              "      <td>0.291512</td>\n",
              "      <td>-0.496996</td>\n",
              "      <td>0.249929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rad</th>\n",
              "      <td>0.625505</td>\n",
              "      <td>-0.311948</td>\n",
              "      <td>0.595129</td>\n",
              "      <td>-0.007368</td>\n",
              "      <td>0.611441</td>\n",
              "      <td>-0.209847</td>\n",
              "      <td>0.456022</td>\n",
              "      <td>-0.494588</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.910228</td>\n",
              "      <td>0.464741</td>\n",
              "      <td>-0.444413</td>\n",
              "      <td>0.488676</td>\n",
              "      <td>-0.381626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tax</th>\n",
              "      <td>0.582764</td>\n",
              "      <td>-0.314563</td>\n",
              "      <td>0.720760</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>0.668023</td>\n",
              "      <td>-0.292048</td>\n",
              "      <td>0.506456</td>\n",
              "      <td>-0.534432</td>\n",
              "      <td>0.910228</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.460853</td>\n",
              "      <td>-0.441808</td>\n",
              "      <td>0.543993</td>\n",
              "      <td>-0.468536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ptratio</th>\n",
              "      <td>0.289946</td>\n",
              "      <td>-0.391679</td>\n",
              "      <td>0.383248</td>\n",
              "      <td>-0.121515</td>\n",
              "      <td>0.188933</td>\n",
              "      <td>-0.355501</td>\n",
              "      <td>0.261515</td>\n",
              "      <td>-0.232471</td>\n",
              "      <td>0.464741</td>\n",
              "      <td>0.460853</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.177383</td>\n",
              "      <td>0.374044</td>\n",
              "      <td>-0.507787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>b</th>\n",
              "      <td>-0.385064</td>\n",
              "      <td>0.175520</td>\n",
              "      <td>-0.356977</td>\n",
              "      <td>0.048788</td>\n",
              "      <td>-0.380051</td>\n",
              "      <td>0.128069</td>\n",
              "      <td>-0.273534</td>\n",
              "      <td>0.291512</td>\n",
              "      <td>-0.444413</td>\n",
              "      <td>-0.441808</td>\n",
              "      <td>-0.177383</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.366087</td>\n",
              "      <td>0.333461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lstat</th>\n",
              "      <td>0.455621</td>\n",
              "      <td>-0.412995</td>\n",
              "      <td>0.603800</td>\n",
              "      <td>-0.053929</td>\n",
              "      <td>0.590879</td>\n",
              "      <td>-0.613808</td>\n",
              "      <td>0.602339</td>\n",
              "      <td>-0.496996</td>\n",
              "      <td>0.488676</td>\n",
              "      <td>0.543993</td>\n",
              "      <td>0.374044</td>\n",
              "      <td>-0.366087</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.737663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medv</th>\n",
              "      <td>-0.388305</td>\n",
              "      <td>0.360445</td>\n",
              "      <td>-0.483725</td>\n",
              "      <td>0.175260</td>\n",
              "      <td>-0.427321</td>\n",
              "      <td>0.695360</td>\n",
              "      <td>-0.376955</td>\n",
              "      <td>0.249929</td>\n",
              "      <td>-0.381626</td>\n",
              "      <td>-0.468536</td>\n",
              "      <td>-0.507787</td>\n",
              "      <td>0.333461</td>\n",
              "      <td>-0.737663</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             crim        zn     indus  ...         b     lstat      medv\n",
              "crim     1.000000 -0.200469  0.406583  ... -0.385064  0.455621 -0.388305\n",
              "zn      -0.200469  1.000000 -0.533828  ...  0.175520 -0.412995  0.360445\n",
              "indus    0.406583 -0.533828  1.000000  ... -0.356977  0.603800 -0.483725\n",
              "chas    -0.055892 -0.042697  0.062938  ...  0.048788 -0.053929  0.175260\n",
              "nox      0.420972 -0.516604  0.763651  ... -0.380051  0.590879 -0.427321\n",
              "rm      -0.219247  0.311991 -0.391676  ...  0.128069 -0.613808  0.695360\n",
              "age      0.352734 -0.569537  0.644779  ... -0.273534  0.602339 -0.376955\n",
              "dis     -0.379670  0.664408 -0.708027  ...  0.291512 -0.496996  0.249929\n",
              "rad      0.625505 -0.311948  0.595129  ... -0.444413  0.488676 -0.381626\n",
              "tax      0.582764 -0.314563  0.720760  ... -0.441808  0.543993 -0.468536\n",
              "ptratio  0.289946 -0.391679  0.383248  ... -0.177383  0.374044 -0.507787\n",
              "b       -0.385064  0.175520 -0.356977  ...  1.000000 -0.366087  0.333461\n",
              "lstat    0.455621 -0.412995  0.603800  ... -0.366087  1.000000 -0.737663\n",
              "medv    -0.388305  0.360445 -0.483725  ...  0.333461 -0.737663  1.000000\n",
              "\n",
              "[14 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-go98WrTa8T9",
        "colab_type": "text"
      },
      "source": [
        "> **Usando** [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QccpxajafBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "74502b24-fe5b-481e-a8c0-c934c043c8d7"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#criando o nosso modelo como todos os parametros \"Default\"\n",
        "reg = LinearRegression()\n",
        "\n",
        "#treinando o model\n",
        "reg.fit(X_Rooms_train, y_train)\n",
        "\n",
        "#Coeficientes\n",
        "print(\"Coeficiente: {}\".format(reg.coef_))\n",
        "print(\"Intercept: {}\".format(reg.intercept_))\n",
        "\n",
        "#salvando nossas predicoes\n",
        "preds_sklearn = reg.predict(X_Rooms_test)\n",
        "\n",
        "#Avaliando o modelo (R^2)\n",
        "print(\"R2: {}\".format(reg.score(X_Rooms_test, y_test)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coeficiente: [9.34830141]\n",
            "Intercept: -36.24631889813795\n",
            "R2: 0.3707569232254778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IlQGNDpLo27",
        "colab_type": "text"
      },
      "source": [
        "> **Plotando os modelos**\n",
        "\n",
        "\n",
        "Caso queira aprender mais sobre matplotlib, temos um [treinamento](https://github.com/nicholasrichers/dataviz-neuron/blob/master/Treinamento_Dataviz.ipynb) específico para essa biblioteca\n",
        "\n",
        "*(crie aqui um um plot usando dos 2 exemplos acima na mesmo gráfico + valores reais)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ojA_7d-NJGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cdd2f3b0-cc47-4288-dec7-7191bbda9e7f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "#Modelo criado manualmente\n",
        "plt.plot(X_Rooms_test, preds_sklearn+1, color=\"blue\")\n",
        "\n",
        "#Modelo criado com sklearn\n",
        "plt.plot(X_Rooms_test, preds_sklearn, color=\"red\")\n",
        "\n",
        "#Valores Preditos\n",
        "plt.plot(X_Rooms_test, y_test, 'ro')\n",
        "\n",
        "\n",
        "#plt.plot(X_Rooms_train, y_train, 'bo')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt_1NkX2bl82",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS2J5U6QfGvH",
        "colab_type": "text"
      },
      "source": [
        "### Regressão Múltipla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9QNbA6uQMXG",
        "colab_type": "text"
      },
      "source": [
        "A  nossa análise pode melhorar muito se incluirmos as demais features em nossa análise, faremos isso usando regressão múltipla. E existem algumas maneiras diferentes paraestimarmos os coeficientes e começaremos falando de duas delas.\n",
        "\n",
        "*(eu gosto desse [post](https://machinelearningmastery.com/linear-regression-for-machine-learning/) sobre o assunto)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQA95N3YP2QK",
        "colab_type": "text"
      },
      "source": [
        "> **Método dos Mínimos Quadrados (Ordinary Least Squares - OLS)**\n",
        "\n",
        "\n",
        "Descrição:\n",
        "\n",
        "Esse procedimento busca minimizar a soma do quadrados dos resíduios e dessa forma estimar uma solução única para o vetor de parâmetros ($b$). Partindo do nosso problema original temos que:\n",
        "\n",
        ">$y=X.b$ (1)\n",
        "\n",
        "Onde $X$ é a nossa matriz de features, $y$ é o preço estimado das casas e $b$, o vetor de coefientes que precisamos estimar. Extendendo a eq. (1), temos que:\n",
        "\n",
        "> $X' . y = X'. X.b $ (2)\n",
        "\n",
        "Onde bastou multiplicartmos ambos os lados pela matriz transposta $(X')$. Em seguida desenvolvendo a eq. (2) algebricamente e isolando $b$, temos:\n",
        "\n",
        "\n",
        "> $b = (X' . X)^{-1} . X' . y$ (3)\n",
        "\n",
        "\n",
        "Note que $(X' . X)^{-1} . X'$, é conhecida como a [pseudo-inversa](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) de $X$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2IW3grv-lWJ",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, usaremos a eq. (3) para estimar nosso vetor $b$, onde:\n",
        "\n",
        "\n",
        "```\n",
        "np.linalg.inv(X_train.T.dot(X_train))\n",
        "```\n",
        "Representa $(X' . X)^{-1} $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0OaE6fGf7aP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2460d5b3-f042-47ca-9d42-ceb558a5bfc7"
      },
      "source": [
        "#Determinando b pelo método OLS, manualmente\n",
        "b = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
        "\n",
        "#Coeficientes\n",
        "print(\"Coeficientes: {}\".format(b))\n",
        "\n",
        "#Gerando as predições\n",
        "preds_ols = X_test.dot(b)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coeficientes: [-0.10047235  0.03417569  0.033571    2.73965227 -5.15511967  6.19597333\n",
            " -0.01108091 -1.020035    0.13881847 -0.00779673 -0.45408744  0.0168434\n",
            " -0.42589164]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6MF_qlSD4gN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IfjW0Bi59qZs"
      },
      "source": [
        "> **Usando skelarn**\n",
        "\n",
        "A função [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), também usa o método OLS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J-rTa6G69qZx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ea7976de-d8ac-406f-cfc0-6ad31ca8c895"
      },
      "source": [
        "# faca o mesmo exemplo de uma variável usando sklearn\n",
        "#criando o nosso modelo como todos os parametros \"Default\"\n",
        "reg_ols = LinearRegression()\n",
        "\n",
        "#treinando o model\n",
        "reg_ols.fit(X_train, y_train)\n",
        "\n",
        "#Coeficientes\n",
        "print(\"Coeficientes: {}\".format(reg_ols.coef_))\n",
        "print(\"Intercept: {}\".format(reg_ols.intercept_))\n",
        "#salvando nossas predicoes\n",
        "preds_ols_sklearn = reg_ols.predict(X_test)\n",
        "\n",
        "#Avaliando o modelo (R^2)\n",
        "print(\"R2: {}\".format(reg_ols.score(X_test, y_test)))\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coeficientes: [-1.13055924e-01  3.01104641e-02  4.03807204e-02  2.78443820e+00\n",
            " -1.72026334e+01  4.43883520e+00 -6.29636221e-03 -1.44786537e+00\n",
            "  2.62429736e-01 -1.06467863e-02 -9.15456240e-01  1.23513347e-02\n",
            " -5.08571424e-01]\n",
            "Intercept: 30.24675099392401\n",
            "R2: 0.6687594935356298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GdOVNKPEEcl",
        "colab_type": "text"
      },
      "source": [
        "> **Comparando nosso modelo OLS criado manualmente com o sklearn**\n",
        "\n",
        "Vemos que as prediçoes de ambos os modelos são na grande maioria bem próximas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrLZWQ7AB07F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0aa512c7-cf3a-4ee2-e3d0-b3a8004718e8"
      },
      "source": [
        "#predições do modelo manual\n",
        "plt.scatter(preds_ols.index, preds_ols, color=\"blue\")\n",
        "\n",
        "#predições do modelo sklearn\n",
        "plt.scatter(preds_ols.index, preds_ols_sklearn, color=\"red\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f801a1878d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bJI5yV0_VIC",
        "colab_type": "text"
      },
      "source": [
        "Vantagens\n",
        "- Fornece uma maneira elegante e direta de implementarmos manualmente usando notação matricial.\n",
        "- A demonstração desse método levará a algumas considerações importantes que usaremos mais a frente nesse estudo.\n",
        "\n",
        "\n",
        "Desvantagens:\n",
        "- Você precisa carregar todo o dataset na memória de uma vez e calcular uma matriz inversa, o que pode ser custoso computacionamente em um dataset grande (não é o nosso caso aqui)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3KBp7lgjGsE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9kDgUM_eIYJ",
        "colab_type": "text"
      },
      "source": [
        "> **Método de Descida Gradiente (Gradient Descent)**\n",
        "\n",
        "*( bom resumo abaixo)*\n",
        "\n",
        "Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\n",
        "\n",
        "Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.\n",
        "\n",
        "\n",
        "When there are one or more inputs you can use a process of optimizing the values of the coefficients by iteratively minimizing the error of the model on your training data.\n",
        "\n",
        "This operation is called Gradient Descent and works by starting with random values for each coefficient. The sum of the squared errors are calculated for each pair of input and output values. A learning rate is used as a scale factor and the coefficients are updated in the direction towards minimizing the error. The process is repeated until a minimum sum squared error is achieved or no further improvement is possible.\n",
        "\n",
        "When using this method, you must select a learning rate (alpha) parameter that determines the size of the improvement step to take on each iteration of the procedure.\n",
        "\n",
        "\n",
        "Gradient descent is often taught using a linear regression model because it is relatively straightforward to understand. In practice, it is useful when you have a very large dataset either in the number of rows or the number of columns that may not fit into memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvenN_K0InR-",
        "colab_type": "text"
      },
      "source": [
        "Gradient Descent\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1baK9QuYh5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Não estou certo se aqui vale a pena criar um exemplo na mao, talvez um gif seja mais explicativo\n",
        "\n",
        "\n",
        "#ESSE POST É Excelente, usa o mesmo dataset e implementa a mesma ideia\n",
        "\n",
        "#https://towardsdatascience.com/linear-regression-from-scratch-with-numpy-implementation-finally-8e617d8e274c"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxUK0FZCwlS_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_O36plXhxtt",
        "colab_type": "text"
      },
      "source": [
        "> Usando Sklearn\n",
        "\n",
        "Como a regressão linear é feita usando o método OLS, aqui vamos usar o [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) ... (explicar a diferenca usando gradiente estocástico)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2RoxRVVf7Wx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "820ab05b-7a26-4483-dca7-08637a2fab1c"
      },
      "source": [
        "# faca o mesmo exemplo de uma variável usando sklearn\n",
        "#criando o nosso modelo como todos os parametros \"Default\"\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "reg_sgd =  SGDRegressor() #LinearRegression()\n",
        "\n",
        "#treinando o model\n",
        "reg_sgd.fit(X_train, y_train)\n",
        "\n",
        "#Coeficientes\n",
        "print(\"Coeficiente: {}\".format(reg_sgd.coef_))\n",
        "print(\"Intercept: {}\".format(reg_sgd.intercept_))\n",
        "\n",
        "#salvando nossas predicoes\n",
        "preds_sklearn = reg_sgd.predict(X_test)\n",
        "\n",
        "#Avaliando o modelo (R^2)\n",
        "print(\"R2: {}\".format(reg_sgd.score(X_test, y_test)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coeficiente: [ 2.22079081e+11 -1.90849870e+11 -4.47311195e+11 -1.80140746e+09\n",
            " -9.07819626e+09  2.65243142e+11  4.17375484e+11  3.05590970e+11\n",
            "  7.31593542e+10 -1.54958418e+11  2.48872019e+11  7.86921956e+10\n",
            " -2.18036104e+11]\n",
            "Intercept: [2.08235236e+10]\n",
            "R2: -1.0484877319454013e+25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWupV8ULiH3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1YZ2QqWFiTgE"
      },
      "source": [
        "> Plotando os modelos\n",
        "\n",
        "*(crie aqui um um plot usando dos 2 exemplos do skelarn)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePRCnkvRiHl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SGDRegressor vs Linear()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtLyXVEzvDYM",
        "colab_type": "text"
      },
      "source": [
        "Além das duas formulações acima, poderíamos estimar os coeficientes estimando a **máxima verossimilhança**, não cobriremos esse método nesse treinamento, mas há uma excelente fonte inicial [aqui](https://machinelearningmastery.com/linear-regression-with-maximum-likelihood-estimation/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdEIaTL0jDsI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY_lAntXi639",
        "colab_type": "text"
      },
      "source": [
        "### Métricas\n",
        "\n",
        "> **Como comparar os modelos usando essas métricas**\n",
        "\n",
        "- Representacao teórica (formula)\n",
        "- Breve explicacao do que ela representa\n",
        "- Possiveis desvantagens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq10kDv0isHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Acho que não precisa criar código manual, usar make_scorer do skelarn a metrica padrao .score() é o r2"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_FONc5-isDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compare os 2 modelos do sklearn, o reg_ols e o reg_sgd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e602-BXkltJ_",
        "colab_type": "text"
      },
      "source": [
        " Parte 1 gabriela até aqui (entre 60 e 75 min)\n",
        " Avaliar se vale a pena fz pausa para perguntas ou descanso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glZo1-DhCbjU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lOErLFIJByFA"
      },
      "source": [
        "> **Intepretação dos modelos (análise dos resíduos)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6R4IRzOqByFD",
        "colab": {}
      },
      "source": [
        "#aqui falar novamente da OLS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1rLdPg8Clyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea1KFR7bCl85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQahBgEWk6LH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPr8xe1Au9wG",
        "colab_type": "text"
      },
      "source": [
        "## Polinomial\n",
        "\n",
        " reg simples com alguma das featutes com corr quadratica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVTT5N64u--N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#colocar aqui exemplo do MLM bias & var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlYF6rxIgz7G",
        "colab_type": "text"
      },
      "source": [
        " Bias & variancia... regularização\n",
        "\n",
        "\n",
        "\n",
        " There are extensions of the training of the linear model called regularization methods. These seek to both minimize the sum of the squared error of the model on the training data (using ordinary least squares) but also to reduce the complexity of the model (like the number or absolute size of the sum of all coefficients in the model).\n",
        "\n",
        "Two popular examples of regularization procedures for linear regression are:\n",
        "\n",
        "Lasso Regression: where Ordinary Least Squares is modified to also minimize the absolute sum of the coefficients (called L1 regularization).\n",
        "Ridge Regression: where Ordinary Least Squares is modified to also minimize the squared absolute sum of the coefficients (called L2 regularization).\n",
        "These methods are effective to use when there is collinearity in your input values and ordinary least squares would overfit the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDk8efM0RIiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lNBaJywsRI9C"
      },
      "source": [
        "Na primeira parte desse treinamento, vamos usar um conjunto de [dados sintético](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression), pois dessa forma podemos explorar algumas propriedades dos modelos de regressão de maneira mais assertiva.\n",
        "\n",
        "*(explicar brevemente  os parâmetros do make_regression, e observar no link acima se há mais parâmetros interessantes para serem usados como bias... )*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uzpjz4hBRI9E",
        "colab": {}
      },
      "source": [
        "#sintetico\n",
        "\n",
        "# generate regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvXtFElkfMRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK0YDooAfMOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7lItUoZf0W6",
        "colab_type": "text"
      },
      "source": [
        "Algoritmos com regularizacao (LASSO, elasticnet), ridge regression, GLM(?)\n",
        "\n",
        "Como ajustar os parametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQYHtq3zfMMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVXeK9PLg5ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa5v_F14g58t",
        "colab_type": "text"
      },
      "source": [
        "Critérios para escolher modelos lineares doc sklearn\n",
        "\n",
        "- dimensionalidade\n",
        "- nivel de ruido\n",
        "- outros...\n",
        "\n",
        "\n",
        "desvantagens de modelos lineares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNvRU2Y9g5Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANEzJ-sXPHbe",
        "colab_type": "text"
      },
      "source": [
        "Modelos de regressao não linear (arvores e knn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaiNVw0ffMJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot knn https://medium.com/analytics-vidhya/k-neighbors-regression-analysis-in-python-61532d56d8e4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQk3XE0JkO5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN60VdvYkO1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsD5OsTMgVoa",
        "colab_type": "text"
      },
      "source": [
        "regressao stepwise\n",
        "\n",
        "*(nao existe de maneira explícita no skelarn mas vc pode usar RFE como diz [aqui](https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm))*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM78UO7OSycl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#aqui usar dado sintético parece uma boa ideia\n",
        "from sklearn.datasets import make_regression\n",
        "# generate regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MbO7viTSyVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8y13bW7fMGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjRGvKewgJ3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wF5dKREgJ9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rZx2I4QgLIZ",
        "colab_type": "text"
      },
      "source": [
        "## Comparacao de modelos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H27uH5YnTvsy",
        "colab_type": "text"
      },
      "source": [
        "**Preparing Data For Linear Regression**\n",
        "\n",
        "Linear regression is been studied at great length, and there is a lot of literature on how your data must be structured to make best use of the model.\n",
        "\n",
        "As such, there is a lot of sophistication when talking about these requirements and expectations which can be intimidating. In practice, you can uses these rules more as rules of thumb when using Ordinary Least Squares Regression, the most common implementation of linear regression.\n",
        "\n",
        "Try different preparations of your data using these heuristics and see what works best for your problem.\n",
        "\n",
        "Linear Assumption. Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n",
        "\n",
        "\n",
        "Remove Noise. Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n",
        "\n",
        "\n",
        "Remove Collinearity. Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n",
        "\n",
        "\n",
        "Gaussian Distributions. Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n",
        "\n",
        "\n",
        "Rescale Inputs: Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GroVORdBHixI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BaoT9As1HjLF"
      },
      "source": [
        "Dataset\n",
        "\n",
        "[link](https://archive.ics.uci.edu/ml/datasets/student+performance), [descrição](https://github.com/nicholasrichers/regressao-neuron/blob/master/student_dataset/student.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "naaZmmg_HjLI",
        "colab": {}
      },
      "source": [
        "#student\n",
        "\n",
        "dados_url = 'https://raw.githubusercontent.com/nicholasrichers/regressao-neuron/master/student_dataset/student-mat.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXF_d2jRhBXI",
        "colab_type": "text"
      },
      "source": [
        "Pycaret // comparacao"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVkWwwxOgJ0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zACh5gxXgOlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbH5vDM2gp6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJbSAgUSuU4Z",
        "colab_type": "text"
      },
      "source": [
        "hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYIjjjmXuFtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Se0id8JkZoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD-mFAN4gPUt",
        "colab_type": "text"
      },
      "source": [
        "Pt 2 até aqui, pausa para perguntas e se houver tempo...\n",
        "\n",
        "Ranking models (extra)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrXXxcYWgOoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-9BakAlgOhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}